# -*- coding: utf-8 -*-
"""Preprocesamiento.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-rBvh6V9SL0zjMM5RPoF0a-TyWh8S4Zd
"""

!python -m spacy download es_core_news_sm

from __future__ import annotations

from pathlib import Path
import re
import spacy

# =========================
# Config
# =========================
INPUT_DIR = Path("/content/corpus")    # corpus original
OUT_DIR   = Path("/content/limpio")    # corpus limpio
OUT_DIR.mkdir(parents=True, exist_ok=True)

# =========================
# spaCy
# =========================
nlp = spacy.load(
    "es_core_news_sm",
    disable=["parser", "ner", "textcat"]
)

# =========================
# Helpers
# =========================
def clean_and_lemmatize(path: Path) -> list[str]:
    """
    Devuelve una lista de líneas limpias:
    - lower
    - sin números ni símbolos
    - sin stopwords
    - lematizadas
    """
    raw_lines = path.read_text(
        encoding="utf-8",
        errors="ignore"
    ).splitlines()

    output_lines = []

    for line in raw_lines:
        line = line.strip().lower()
        if not line:
            continue  # (1) líneas vacías

        # (3) quitamos números y símbolos (dejamos letras y espacios)
        line = re.sub(r"[^a-záéíóúüñ\s]", " ", line)
        line = re.sub(r"\s+", " ", line).strip()
        if not line:
            continue

        doc = nlp(line)

        lemmas = []
        for tok in doc:
            if tok.is_stop:
                continue
            if not tok.is_alpha:
                continue
            lemma = tok.lemma_.strip()
            if lemma:
                lemmas.append(lemma)

        if lemmas:
            output_lines.append(" ".join(lemmas))

    return output_lines

# =========================
# Main
# =========================
files = sorted(INPUT_DIR.glob("*.txt"))
if not files:
    raise FileNotFoundError(f"No hay archivos .txt en {INPUT_DIR}")

for p in files:
    cleaned_lines = clean_and_lemmatize(p)

    out_path = OUT_DIR / p.name
    with out_path.open("w", encoding="utf-8") as f:
        for ln in cleaned_lines:
            f.write(ln + "\n")

    print(f"✔ {p.name}: {len(cleaned_lines)} líneas limpias")

print(f"\nCorpus limpio listo en: {OUT_DIR}")

import shutil
from pathlib import Path

LIMPIO_DIR = Path("/content/limpio")
ZIP_PATH   = Path("/content/limpio_corpus.zip")

# Si ya existía, lo borramos
if ZIP_PATH.exists():
    ZIP_PATH.unlink()

shutil.make_archive(
    base_name=ZIP_PATH.with_suffix(""),
    format="zip",
    root_dir=LIMPIO_DIR
)

print(f"ZIP creado en: {ZIP_PATH}")