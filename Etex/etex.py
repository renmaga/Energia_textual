# -*- coding: utf-8 -*-
"""etex.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13v6zxviU1BwzdoqTs_kR5ko6yasKVKrN
"""

#Librerías
from __future__ import annotations

from pathlib import Path
from collections import Counter
from typing import Literal

import numpy as np
import pandas as pd

import shutil
from pathlib import Path

# Config
INPUT_DIR = Path("/content/concat")
OUT_ROOT  = Path("/content/out_etex")
DTYPE = np.float32

# Tokenizaciones
TokenMode = Literal["word", "char"]
NGRAM_LIST = [1, 2, 3, 5, 10]

# IO (lista de strings)
def read_lines(path: Path) -> list[str]:
    return [ln.strip() for ln in path.read_text(encoding="utf-8", errors="ignore").splitlines() if ln.strip()]

# Tokenización
def word_ngrams(tokens: list[str], n: int) -> list[str]:
    if n <= 1:
        return tokens
    if len(tokens) < n:
        return []
    return [" ".join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]

def char_ngrams(text: str, n: int) -> list[str]:
    text = text.replace(" ", "")
    if n <= 1:
        return list(text)
    if len(text) < n:
        return []
    return [text[i:i+n] for i in range(len(text) - n + 1)]

def tokenize_lines(lines: list[str], mode: TokenMode, n: int) -> list[list[str]]:
    if mode == "word":
        return [word_ngrams(ln.split(), n) for ln in lines]
    if mode == "char":
        return [char_ngrams(ln, n) for ln in lines]
    raise ValueError(f"Unknown mode: {mode}")

# Vocab + matrices

def build_vocab(tokens_per_line: list[list[str]]) -> tuple[list[str], dict[str, int]]:
    vocab = sorted({tok for toks in tokens_per_line for tok in toks})
    return vocab, {t: i for i, t in enumerate(vocab)}

def build_S(tokens_per_line: list[list[str]], tok2idx: dict[str, int]) -> np.ndarray:
    # S: (n_lines x vocab)
    S = np.zeros((len(tokens_per_line), len(tok2idx)), dtype=DTYPE)
    for i, toks in enumerate(tokens_per_line):
        if not toks:
            continue
        for tok, c in Counter(toks).items():
            S[i, tok2idx[tok]] = c
    return S

def compute_E(S: np.ndarray) -> np.ndarray:
    # Etex: (n_lines x n_lines)
    return (S @ S.T).astype(DTYPE)

def compute_C(S: np.ndarray) -> np.ndarray:
    # Cooc: (vocab x vocab)
    return (S.T @ S).astype(DTYPE)

def normalize_by_max_abs(M: np.ndarray) -> np.ndarray:
    m = float(np.max(np.abs(M))) if M.size else 0.0
    return (M / m).astype(DTYPE) if m != 0.0 else M.astype(DTYPE)

# archivos

def clean_stem_no_underscores(path: Path) -> str:
    return path.stem.replace("_", "")

def tokenization_label(mode: TokenMode, n: int) -> str:
    return f"p{n}" if mode == "word" else f"c{n}"

def save_csv(matrix: np.ndarray, out_path: Path) -> None:
    out_path.parent.mkdir(parents=True, exist_ok=True)
    pd.DataFrame(matrix).to_csv(out_path, index=False, header=False)

# Core por archivo

def get_mats(in_path: Path, out_root: Path, mode: TokenMode, n: int, stats_rows: list[dict]) -> None:
    tok_label = tokenization_label(mode, n)

    lines = read_lines(in_path)
    tokens_per_line = tokenize_lines(lines, mode=mode, n=n)

    vocab, tok2idx = build_vocab(tokens_per_line)
    S = build_S(tokens_per_line, tok2idx)

    # Matrices
    C = compute_C(S)                # coocurrencias
    E = compute_E(S)                # etex
    En = normalize_by_max_abs(E)    # normalizada

    stem_clean = clean_stem_no_underscores(in_path)

    # Guardado
    save_csv(S,  out_root / "ocur"  / tok_label / f"ocur_{stem_clean}.csv")
    save_csv(C,  out_root / "cooc"  / tok_label / f"cooc_{stem_clean}.csv")
    save_csv(E,  out_root / "etex"  / tok_label / f"etex_{stem_clean}.csv")
    save_csv(En, out_root / "enorm" / tok_label / f"enorm_{stem_clean}.csv")

    stats_rows.append({
        "file": in_path.name,
        "stem_clean": stem_clean,
        "tokenization": tok_label,
        "mode": mode,
        "n": n,
        "n_oraciones": len(lines),
        "vocab_size": len(vocab),
        "total_tokens": int(sum(len(toks) for toks in tokens_per_line)),
    })

# Batch

def run_batch(input_dir: Path, out_root: Path, mode: TokenMode, n: int) -> None:
    tok_label = tokenization_label(mode, n)
    txt_files = sorted([p for p in input_dir.glob("*.txt") if p.is_file()])
    if not txt_files:
        raise FileNotFoundError(f"No .txt files found in {input_dir}")

    stats_rows: list[dict] = []

    for p in txt_files:
        get_mats(p, out_root, mode, n, stats_rows)

    stats_path = out_root / "stats" / f"stats_{tok_label}.csv"
    stats_path.parent.mkdir(parents=True, exist_ok=True)
    pd.DataFrame(stats_rows).to_csv(stats_path, index=False)

    print(f"OK: {tok_label} ({mode}, n={n}) -> {len(txt_files)} files | stats: {stats_path}")

# Run all

OUT_ROOT.mkdir(parents=True, exist_ok=True)

for mode in ("word", "char"):
    for n in NGRAM_LIST:
        run_batch(INPUT_DIR, OUT_ROOT, mode, n)

print("\nDONE. Outputs en:", OUT_ROOT)

# archivos en zip

CONCAT_DIR = Path("/content/out_etex")
ZIP_PATH   = Path("/content/etex.zip")

if ZIP_PATH.exists():
    ZIP_PATH.unlink()

shutil.make_archive(
    base_name=ZIP_PATH.with_suffix(""),
    format="zip",
    root_dir=CONCAT_DIR
)

print(f"ZIP creado en: {ZIP_PATH}")